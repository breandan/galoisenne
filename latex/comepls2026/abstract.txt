Bounded Resources as Languages: A Grammatical Embedding of Substructural Constraints

Context-free grammars (CFGs) occupy a sweet spot between expressiveness and tractability: they admit precise closure properties (e.g., intersection with regular languages), sharp algorithmic bounds (subcubic parsing or polylog parallel DAG reachability), and unusually strong enumerability results. In particular, classical results from formal language theory (e.g., the Bar-Hillel theorem and Chomsky-Sch√ºtzenberger enumeration theorem) yield algebraic generating functions, which enable model counting, exact sampling and complexity guarantees that remain tractable under finite intersections.

In contrast, substructural logics (e.g., linear, affine, relevant) approach "language" through a different lens: rather than discussing complexity theory, they emphasize resource usage via mechanisms for controlling duplication, disposal, and protocol state. This viewpoint has become central to the design of modern type systems, yet is rarely connected back to the combinatorics that make CFGs so effective as search spaces. As a result, resource constraints are often treated syntactically without a principled account of their denotational semantics, langauge-theoretic structure, or the asymptotic cost of witnessability.

This talk proposes a synthesis: we will show an embedding from resource specifications to CFGs that preserves modular structure and composability, and yields product constructions with explicit spacetime bounds. Under our embedding, resource-aware judgments become grammatical derivations, recovering the algorithmic benefits of CFL reachability: exact slice counting, witnessability, and efficient uniform or autoregressive sampling. This reframes substructural logics as finitely sliceable languages, fusing proof search with analytic combinatorics, finite model theory, and syntax-guided program synthesis.