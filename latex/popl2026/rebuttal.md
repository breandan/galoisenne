
We thank all three reviewers for carefully reading our manuscript. We are glad the reviewers seemed to grasp the basic technique, and recognized it was "interesting," (#257A) "compelling," (#257B) and "elegant" (#257C) but regret they found the paper difficult to read, particularly since considerable effort was devoted to making it accessible with step-by-step examples and illustrations. In any case, we appreciate their encouragement and well-intentioned feedback.

All three reviewers expressed some confusion about the significance of the porous completion problem, so let's start by clarifying that point, then we will address the specific concerns of each reviewer.

Porous completion is a general technique for abstract parsing with holes that we have found useful for many problems involving CFGs. It is specifically necessary in this paper to construct the parameterized Parikh map (PPM), as we explain in 4.6, equation (10).

> To obtain the parameterized Parikh map (PPM) of a length-bounded CFG, we abstractly parse the porous string and take the minimal cover of all intervals, which subsumes the Parikh image of every repair in the Levenshtein ball.

To restate this in more explicit terms, we precompute $\mathbb{T}_3$ for porous strings up to some large number, i.e., {`_`, `_ _`, `_ _ _`, ...}. Recall $\mathbb{T}_3$ is simply a dictionary mapping each participating nonterminal $v_i: V$ to a $T: \mathbb{T}_2$, the ADT representing all parse forests sharing the root $v$. This will produce a lookup table of string lengths and nonterminals to $\mathbb{T}_2$ instances. 

```
      V      v1       v2        v3     …
|σ|
 1       T[1, v1]  T[1, v2]  T[1, v3]
 
 2       T[2, v1]  T[2, v2]  T[2, v3]  …
 
 3       T[3, v1]  T[3, v2]  T[3, v3]
 
 ⋮                   ⋮                 ⋱
```

Each instance $T[i, v]$ can then be translated to a Parikh image, which has type $π: \{[a, b] \in \mathbb{N} \times \mathbb{N} \mid a \leq b\}^{|\Sigma|}$. So we can replace the above table with the following table, by applying Eq. 7 in Sec. 4.6 elementwise to each $T[i, v]$:

```
      V      v1       v2        v3     …
|σ|
 1       π[1, v1]  π[1, v2]  π[1, v3]
 
 2       π[2, v1]  π[2, v2]  π[2, v3]  …
 
 3       π[3, v1]  π[3, v2]  π[3, v3]
 
 ⋮                   ⋮                 ⋱
```

If we unpack a given $π[...]$, it will contain a vector of intervals for each terminal in the alphabet, whose lower and upper bounds represent the minimum and maximum number of times that terminal can be generated by the nonterminal $v$ in a string of length $i$, e.g., assuming the alphabet is $\{a, b, c\}$:

```
π[3, v1] = {a: [1, 2], b: [0, 1], c: [0, 0]}
```

Now, we can compute the PPM for any range of string lengths $[m, n]$ by summing over the pairs of intervals. Flattened, it stores the following:

```
      V          v1                          v2                 …
m-n 
1-1     T[1, v1]                    T[1, v2]  
 
1-2     T[1, v1]⊕T[2, v1]           T[1, v2]⊕T[2, v2]
 
1-3     T[1, v1]⊕T[2, v1]⊕T[3, v1]  T[1, v2]⊕T[2, v2]⊕T[3, v2]
 
 ⋮                      ⋮                 ⋮                      ⋱
```

We can think of this full PPM as a 4D lookup table, $π(v: V, m: \mathbb{Z}, n: \mathbb{Z}, s:\Sigma) \rightarrow \{[a, b] \in \mathbb{N} \times \mathbb{N} \mid a \leq b\}$. It stores the minimum and maximum number of each terminal that each nonterminal must and can generate, assuming it generates a string whose length falls between $m$ and $n$. This can be computed once for each grammar and cached for later use in the modified Bar-Hillel construction.

Suppose we are given a broken string of length $|\sigma| = 5$, and we want to search a Levenshtein radius of $d = 2$. Then we would look up the parameterized Parikh map between length 3 and 7, representing the minimum and maximum number of each terminal generated, across all strings in the nonterminal's language between length 3 and 7.

This optimization eliminates the vast majority of all useless productions from the synthetic intersection grammar and allows us to scale up the Bar-Hillel construction to real-world syntax repairs, making it tractable to construct for large grammars (e.g., Python) and Levenshtein automata (~80x4 states). This contribution is the crux of the paper without which it would be impossible (or at least very difficult) to implement repairs in the manner described.

Finally, we would like to point out an oversight the reviewers missed in the submitted manuscript. The pairing function (Eq. 14) should have been defined as follows:

$$
  \varphi(T: \mathbb{T}_2, i: \mathbb{Z}_{|T|}) \mapsto \begin{cases}
  \texttt{BTree}\big(\texttt{root}(T)\big) & \text{if $T$ is a leaf,} \\
  \textbf{let } F(n) = \sum_{\langle l, r\rangle \in \texttt{children}[0 \ldots n]}|l|\cdot|r|,\\
  \phantom{\textbf{let }} F^{-1}(u) = \inf \big\{x \mid u \leq F(x)\big\},\\
  \phantom{\textbf{let }} t = F\big(F^{-1}(i)\big),\\
  \phantom{\textbf{let }} q = i - t,\\
  \phantom{\textbf{let }} l, r = \texttt{children}[t],\\
  \phantom{\textbf{let }} q_1, q_2 = \big\langle\lfloor\frac{q}{|r|}\rfloor, q \pmod{|r|}\big\rangle,\\
  \phantom{\textbf{let }} T_1, T_2 = \big\langle\varphi(l, q_1), \varphi(r, q_2)\big\rangle \textbf{ in } \\
  \texttt{BTree}\big(\texttt{root}(T), T_1, T_2\big) & \text{otherwise.} \\
  \end{cases}
$$

Review #257A
===========================================================================

- This paper was painfully difficult to read... makes assumptions about the readers background that are unjustified even at a conference like POPL.

We are pained to hear this. It was our intention to make the paper accessible to a broad audience, and assumed only a basic familiarity with formal languages, compulsory in most undergraduate computer science curricula. If this is not a reasonable assumption for POPL, we will consider submitting our work to a more theoretical venue.

- The compute needed and the latency of this technique seems prohibitive for normal use in developer tooling.

The end-to-end (training and test-time) compute needed for our syntax repair method is orders-of-magnitude lower than required by large language models while offering competitive or even state-of-the-art precision at comparable latency. Our work shows that it is possible to compute the language intersection both exactly and approximately on commodity hardware, and we are confident the method can be used in developer tooling on an off-the-shelf CPU. As the method we propose is embarrassingly parallelizable, it should be possible to attain significant speedups on large repair instances with a dedicated GPU implementation. 

Please see our response to Reviewer #257C (What "fraction"?) for a back-of-the-envelope calculation of our method's energy consumption relative to Seq2Parse and BIFI.

### Detailed comments

- I don't understand what the arrow between "Code edits" and "Edit automaton" refers to.

This arrow indicates the edit automaton generates all single-token code edits. Strictly speaking, the only input to the edit automaton is the alphabet ($\Sigma$) and the original source code snippet ($\sigma$). The arrow here is not a transformation but depicts an equivalence relation; the set of code edits in Fig. 1 is simply artistic embellishment and is never actually materialized.

- The example at the top of Section 2 is appreciated but the details in the rest of this section are inscrutable at this point in the paper... What is the notation [= (] on the right hand side?

It is not so important to understand the details of the construction at this point, here we are just sketching a high-level running example. These details will become apparent in the subsequent formal presentation. It should be clear that [...] is simply a decorative alteration that removes adjacent arcs in the automaton.

- Definition 3.1 is similarly very difficult to understand without defining a number of other concepts from prior work... what is the notation $\sigma: \bar\ell$...?

Definition 3.1 only uses elementary set-theoretic notation and assumes a basic understanding of Levenshtein distance, which we feel is bare minimum expectation for the reader. $\sigma: \bar\ell$ is standard notation for set complement, i.e., $\bar\ell = \Sigma^* - \ell$.

- Section 4.2 seems to assume the reader already understands how a Levenshtein automaton is constructed and what the notation in FIgure 4 means. What [sic. (why?)] does the premise $q_{i,j}$ appear by itself in the DONE rule?

These remarks are helpful, thank you. $q_{i,j}$ should read $q_{i,j} \in Q$. Sec. 4.2 is somewhat terse, but we feel with a close reading it should be clear to a pragmatic reader how to reproduce the ε-free construction. To recapitulate the full Levenshtein automata construction from Schultz and Mihov (2002) would be pedantic, so we focused on the novel aspects of our construction. We will consider adding a brief overview of the Levenshtein automata construction to the introduction.

- 732: why Seq2Parse and BIFI? what do they do (explain at high level here for purposes of interpreting benchmark comparisons even if explained in related work later)? were other alternatives considered? what about Diekmann and Tratt's Don't Panic paper?

Seq2Parse and BIFI are both neural program repair models and approximately target the same problem, but use neural language models. They are the most recent and relevant models in neural program repair.

Diekmann and Tratt's (2020) paper does not handle arbitrary CFGs, nor guarantee completeness across all possible repairs in all locations, only a fixed location. It is only designed to work only with LR parsers. It could be a weak baseline to compare against for Python however the method they present would have an unfair disadvantage in our comparison as it has no notion of "training data" or "naturalness" of a repair sequence.

- 775-781: describes "stability" of dataset repairs. why should we care about this?

Stability is an interesting concept as it relates to error localization. It tells us how much of the string can be modified in any repair of a given Levenshtein distance, or the degree of "caret freedom" in the repair.

- 741-752: different machines used for tidyparse compared to related work? no explanation?? 150GB RAM for tidyparse???

The neural techniques are designed to be executed on fundamentally different hardware architectures – in fact, we even gave an advantage to the neural repair models by evaluating on GPUs which have significantly more processing power. Please see our reply to Reviewer #257C (What "fraction"?) for a more detailed comparison of the energy consumption of our method relative to Seq2Parse and BIFI. We evaluated on both a standard MacBook and a server to simulate an offline and server deployment of the repair system.

- "we rebalanced the...dataset...across each length interval and edit distance..." what does this mean? is data pruned?

Yes, data is pruned to ensure that each length interval and edit distance bucket contains an equal number of repairs. This corrects for imbalances in the number of representatives from each interval in the natural repair dataset. For example, if we just have two intervals, one with 1000 repairs and the other with 100, we would randomly sample 100 repairs from the first interval to match the size of the second.

- Fig 12: presumably being described in 853-866, but missing explicit reference. caption says 30s while text says 10 seconds. if the goal is to measure throughput, the plot suggests throughput increases as edit distance goes up, but surely that's just a function of there being fewer possible repairs at all for fewer edits?

The text should read 30s, although this is the global timeout and each trial does not necessarily consume the whole time alloted - indeed, as result of there being fewer repairs to enumerate. This is the advantage of using a generating function: it serves as a "progress bar" and indicates once there are no further repairs to enumerate so one can terminate early.

Review #257B
===========================================================================

The content of this review is the most insightful of the three, irrespective of the expertise or merit assigned. We want to thank this reviewer for their open-minded curiosity, willingness to engage with the content beyond a cosmetic level, and thoughtful feedback.

- Difference between $2^V$ and $\mathbb{Z}_2^{|V|}$ is that the former has the subset ordering and the latter has the lexicographic ordering?

$2^V$ has no implicit ordering, while the bitvector is ordered lexicographically, however, the choice of ordering is arbitrary. The main difference is that a set requires a high-level programming language and conceals a lot of implementation complexity. The trick here is, we can compute the same CYK function using a low-level bitvector, which is highly efficient and easily translated to propositional logic. There is a neat correspondence between Boolean matrix multiplication and recognition, which enables us to decide CFL membership subcubic time, however, the speedup is not as significant as one might hope due to large constant factors.

We are sorry to hear the reviewer struggled with the matrix parsing concept, this was not our intent. The high level idea behind Section 4.4 is just to show the reader how the $\otimes, \oplus$ algebra can be redefined over different carrier sets. This is the beauty of the semiring programming framework -- we can define the algebra over sets, bitvectors, formulas, automata, intervals, regular expressions and other algebraic datatypes to propagate interesting things. Instead of implementing dozens of parsers and generators, we can simply overload the algebraic operations $\otimes, \oplus$ ("join" and "aggregate", if you prefer) and implement a single ordinary matrix multiplication procedure over an abstract algebra. It is also practically used to compute the PPM as described in Section 4.6.

**Question**: Is the datatype this, in OCaml notation (where `sigma` is the type of letters in the alphabet, and `v` is the type of non-terminals):

```
type p = Leaf of sigma | Node of v * (v * v * p * p) list
```

Yes, this is the appropriate type declaration in OCaml.

**Question** Line 461: What is the 'a' parameter in 'P' for? It doesn't seem to be used?

For the purposes of this presentation $\mathbb{T}_2$ is effectively nonparametric, however in practice a parametric type $a$ can be used to propagate other aggregate statistics, such as the PPM or $|T: \mathbb{T}_2|$. We could have equivalently written $P(a) = \Sigma + aVL\big(V^2 P(a)\big)$ to make this explicit, but felt it was unnecessary to use a type parameter in this case.

**Question** I didn't fully understand the relationships between $\mathbb{T}_2$ and $P$? The former seems to allow an infinite number of children, but the latter only a finite number?

If we assume $\mathbb{T}_2$ is deduplicated then the number of children is upper bounded by $|V|^2$. If we do not assume deduplication, it can be infinite. It occurs to us that $P(a)=\ldots$ is also a slightly confusing notation, as it clashes with the notation for productions ($P$) from the definition of a CFG (L237). Perhaps it is better to just think of $\mathbb{T}_2$ and $P$ as type synonyms, i.e., `type t2 = p` in OCaml.

**Question** Line 486: The text talks about parse forests, but a naive interpretation of the algebraic datatype wouldn't necessarily preserve sharing?

The ADT definition is somewhat dissatisfying as the only constraint it enforces is:

(0) the CNF property of no mixed terminals and nonterminals, i.e., we want to forbid `S -> Bq`, where `B` is a nonterminal and `q` is a terminal. 

The ADT does not enforce sharing, rather it is maintained externally by the implementation of $\otimes, \oplus$, which groups by the roots and aggregates their children at each step. A naive interpretation of the ADT does not impose any constraints except (0), but three side conditions are necessary to be semantically meaningful in the context of CFG parsing:

(1) it must conform to the productions of CFG in CNF, i.e., if `S -> BC` is the only productions whose left-hand side is `S`, then the following tree will typesafe, but semantically meaningless:

```
       S    
      / \
     C   C   
   ...   ...
```

(2) the root of each pair of subtrees must match the children, e.g., both of these examples type check, but only the first is semantically meaningful:

```
   Allowed         Disallowed
      S                S
      |                |
     B C              B C
    /   \            /   \
   B     C          D     F
  ...   ...        ...   ...
```

(3) no duplicate pairs are allowed, e.g.:

```
 Disallowed
      S    
    /   \  
   BC   BC 
  ...   ...
```

Currently, these constraints are enforced at the value level by the implementation of $\oplus, \otimes$. If the reviewer knows of a way to enforce these constraints at the type level, we would be interested in discussing it further.

> This section shows how to use the representation of parse trees to compute the Parikh map of a CFG for finite strings by first parsing the porous string in the algebraic datatype domain, and then counting the non-terminal occurences in the string.

**Question** Isn't this possible without going through the tree representation?

Yes, it is possible to propagate the PPM directly without first computing the intermediate tree structure -- in fact, this is exactly how we implement it. However, it may be more intuitive for the reader to think of as abstractly parsing the all-holes string to first build the parse chart containing $\mathbb{T}_3$, then constructing the PPM by defining an induction over $\mathbb{T}_2$.

The tree representation is really only necessary for sampling and decoding.

**Question** The Levenstein automaton construction and the intersection construction are both phrased as inference rules in this work, which indicates that there might be a way of expressing the problem using Logic Programming. Would it be possible to phrase the problem described here as a instance of probabilistic Datalog or Answer Set Programming? Would existing tools for this be useful?

Absolutely! Datalog and ASP are both more expressive than CFGs, so encoding the problem declaratively is relatively straightforward. Previously, we implemented the problem a number of ways declaratively using an interactive SAT solver, but found it to be very slow. One advantage of using Datalog would be to leverage the newer incremental solvers instead of implementing a custom incremental solution.

The downside of having a separate solver is the communication overhead needed to translate the problem to Datalog/ASP, send it off to the solver, then decode the solution back into the host language is prohibitive. While possible, steering the solver from an embedded DSL is clunky. We want to run this thing to run in realtime on an IDE, so generally the less communication, the better.

Another downside of declarative approaches is there are almost no parallel solvers. We rely heavily on parallelism to make the problem tractable, and would need to investigate how to parallelize Datalog or ASP solvers to make them competitive. We also need the flexibility to define probability distributions over formal languages and customize the sampling order. Fusing probabilistic programming with Datalog / ASP is still an active research problem, so we opted to write a custom string solver, giving us the most implementation flexibility.

Review #257C
===========================================================================

- Why does it matter that other approaches are “trained on a much larger dataset” (line 809)? The claim of “little to no training data” (line 957) seems overblown.

Our method requires zero training data to solve the language intersection problem and uses only a small amount (~100MB) of unlabeled Python text to rerank the repairs for competitive single-shot precision against LLMs trained on gigabytes of text. We stand by this claim and maintain its accuracy.

- What “adapt”ations did you have to perform (line 1135)? 

Hypergraphs, ADTs and CFGs are all basically interchangeable. We prefer ADTs for their name and type checking properties, however, using a hypergraph to represent the grammar is also possible if you like keeping track of tensor indices. For finite languages, all three representations can be collapsed to a DFA with a manageable increase in space complexity.

- In what sense is your approach “complementary” (line 1136, 1146) to existing work, rather than duplicating it?

Our approach is complementary in the sense that it focuses on language intersections. The decoding goal is roughly the same, but we can solve the exact top-k max decoding problem in the case of finite CFLs, whereas their work focuses on approximate top-k max decoding.

- What kind of user has the patience to wait 30 seconds to edit a few tokens? Yet waiting several hours does not “tax the patience of most users” of StackOverflow (line 878).

30s is the upper bound, most repairs do not take this long. It should be possible to accelerate this with a more efficient GPU implementation.

- The beginnings of Section 2, Section 3, and Section 4 end up repeating the same broad strokes painting a picture shrouded in the same mystery as to what all this optimization and sampling is for.

The purpose of "all this optimization and sampling" is to find the most natural repair. The problem is informally stated in Section 2 by means of a clear example, formally restated in Section 3, then Section 4 introduces the method for solving the problem. It is not clear how the problem being solved could be any less mysterious.

- Why is higher throughput a good thing, if each suggestion generated requires the programmer’s attention to accept or reject? It seems better to suggest fewer repairs if one of them is correct.
- Line 204: Why “retrieve as many repairs as possible”?

The premise of this question is mistaken. At no point do we suggest displaying each repair to the user. The entire purpose of reranking repairs is so that the user only needs to review the top-n most likely suggestions (where n is a small number, e.g., 1, 5 or 10). Optimizing for unique repair throughput gives the ranking function the best chance of surfacing the intended repair to the top of the list. We explain this clearly in the paper:

> Although latency and precision are ultimately the deciding usability factors, repair throughput is a crucial intermediate factor to consider when evaluating the performance of a repair system. Even with a perfectly accurate scoring function, if the correct repair is never retrieved, it will be for naught. By maximizing the total number of unique valid repairs, we increase the likelihood of retrieving natural repairs to give the scoring function the best chance of ranking them successfully. For this reason, we prioritize throughput heavily in our design and evaluation.

- Line 972: What is a concrete example of “small or contextually unlikely repairs, which may be overlooked by neural models”, that motivates your approach?

Please refer to Example 1 in Section 2 and Appendix A. Our experiments demonstrate that even BIFI, the SoTA neural model with 20k test-time samples, is only able to retrieve the ground-truth single-edit repair roughly 60-70% of the time. Given up to 10k samples, our method will always retrieve the ground-truth repair.

- Line 102 “We use a variant”: Why?
- Line 124 “our variation”: Why vary?

Both of these adaptations are key optimizations that enable us to efficiently represent the language intersection by reducing the number of synthetic productions created.

- Line 260: Why “more amenable to our purposes”?

We encourage the reviewer to peruse Pasti et al. (2023), who describe how ε-arcs complicate matters. The original Bar-Hillel construction (OBHC) is defined over ε-free automata, whilst the original Levenshtein automata defined by Schulz and Mihov (2002) contains ε-arcs. We overcome this obstacle by redefining an equivalent ε-free automaton and adhere to the OBH construction for CFL intersection, then specialize it to intersections with ε-free Levenshtein automata, which we call the Levenshtein-Bar-Hillel (LBH) construction.

- What use of this approach determines whether it is “viable” or not? What does “converges extremely poorly” mean, and why is that bad if the language contains both natural and unnatural repairs?

Convergence in terms of how quickly the retrieved set approaches the intersection language relative to the number of samples drawn. Ideally, we want the algorithm to return every repair in the Levenshtein ball, but will settle for a representative subset under hard real-time constraints. Top-down sampling is not viable because the ancestral sampler described frequently draws duplicates and heavily biases likely productions close to the `S` symbol. 

For illustrative purposes, let us consider a pathological case for top-down ancestral (TDA) sampling:

```
S -> A B (999/1000)
S -> C C (1/1000)
A -> a (1)
B -> b (1)
C -> a (1/26) | ... | z (1/26)
```

TDA sampling will almost always generate the string `a b`, but most of the language's probability mass is concentrated in the hidden branch, `S -> C C`. Although a contrived example, it illustrates precisely why TDA sampling is unviable: we want a sampler that matches the true distribution over the finite CFL, not the PCFG's local approximation thereof.

On average, TDA sampling will require asymptotically many more samples relative to the tree sampler to enumerate the finite CFL represented by an acyclic PCFG.

- Line 634, 930: How do “large finite CFLs and language intersections involving highly ambiguous CFGs” arise in “practice” (line 641) with Python?

As we explicitly demonstrated in Sec. 4.9, if the NFA is ambiguous, then the BH intersection with an unambiguous CFG can be ambiguous. The Lev-NFA is ambiguous, thus, the intersection with the CFG (Python) can be ambiguous. What this means in practice is that the same repair can be parsed in more than one way by the LBH intersection grammar, which means it can be generated more than once by an integer-tree bijection.

- Line 973: Without specifying the use case, how do you know that “latency and precision are ultimately the deciding usability factors”, or that “repair throughput is a crucial intermediate factor”?

Latency and precision (in either order) are obviously the most important usability factors of any program repair system, irrespective of the desired use case. Throughput is important when the error-correction problem is underdetermined, for the reasons previously discussed.

### Unfamiliar terms

- **Lexical**: The string of lexed tokens. This takes an arbitrary string of Unicode tokens, and transforms it in into a string whose alphabet is the CFG terminals.
- **Nominal** refers to the theory of universals originating from 13th century theologian and logician William of Occham, the founder of nominalism. More recently, it was taken up in the context of programming languages by Gabbay and Pitts (2002) under the auspices of "nominal sets" to refer to Fraenkel-Mostowski set theory, then adopted by Bojańczyk et al. (2012) who developed a theory of automata based on nominal sets. In the context of this paper, it simply denotes an automaton with arcs that accept or reject a terminal, without the orbit-finiteness criteria. A variant is studied under the term "symbolic automata" by D'Antoni et al. (2014, 2017), however, we prefer the nominal formalism as it is explicitly designed to handle name binding semantics in a programming language. In this particular paper, nominalization effectively reduces the number of arcs in the Levenshtein automaton.
- $\ell_\cap$ is another name for the intersection language, i.e., $A$ (for admissible set). We use this notation interchangeably.
- The **generating function**, as its name implies, is a function that generates the set, e.g., maps integers to unique members of the language. We refer the reviewer to *generatingfunctionology* (Wilf, 1990) for a more thorough discussion of this topic.
- The trajectories through the DFA generating $\ell_\cap$. Each trajectory originates from a start state and terminates with an accepting state.
- The **Parikh image** or **vector** is essentially a histogram, i.e., the number of occurrences of each terminal in a string. It comes from Rohit Parikh's work, "On Context-Free Languages" (1966). Our usage refers to an n-hyperrectangle or histogram with whiskers whose lower and upper bounds define the minimum and maximum number of terminals a given nonterminal must and can respectively generate. This was in fact defined on L318-319:

> we write $\mathbb{N}^*=\mathbb{N} \cup \{\infty\}$ to denote the upper bound, and $\Pi = \{[a, b] \in \mathbb{N} \times \mathbb{N}^* \mid a \leq b\}^{|\Sigma|}$ to denote the Parikh image of all terminals. 

- **BIFI tokens** are tokens from the BIFI dataset (Yanasuga et al., 2021).
- **Caret freedom** is a property that describes the percentage of indices that are free to be edited by an n-edit patch, resulting in a syntactically valid repair. If a code snippet has caret freedom of 0.5 on 2-edit repairs, then 50% of the indices contain an edit that will result in a syntactically valid repair.
- PCFGs are first-order models, **higher-order nonterminal dependencies** are generalizations of the PCFG formalism that consider interactions between more distant nonterminals in the parse tree, e.g., probabilistic higher order grammars (Bielik et al., 2016).
- In line 958: **flexibility** refers to the fact that our method works for any CFG and code snippet, and **controllability** essentially means it can be steered by any autoregressive language model that yields a distribution over next tokens.
- **Language equations** are systems of equations over formal languages, see Ginsburg and Rice (1962) for a more thorough discussion of this topic.

### Minor comments

- Line 65: Isn’t possibility (4) just the original?

No. Tokens highlighted in red are deletions, so (4) would be `v = df.iloc(5, 2)`.

- Line 139: What is “the nonterminal I”?

This is an error, it should instead read as follows:

> Likewise, the nonterminal $q_{i, j}Fq_{i+1, j}$ is clearly impossible, as the nonterminal $F$ requires at least three tokens, and the only path from $q_{i, j}$ to $q_{i+1, j}$ has length one.

- Line 805, 968: What "fraction"?

It is tricky to precisely quantify how much compute is used by Seq2Parse (~250W NVIDIA GTX Titan X) and BIFI (~350 W GeForce RTX 3080) during training, but assuming a conservative estimate of 25 kWh per neural training run, our method consumes ~$\frac{1}{1500}$ of the energy for training (~100W CPU * 10 min) and ~$\frac{2}{5}$ the energy for test-time inference, assuming comparable CPU/GPU utilization and latency targets. The total amount of energy consumed by our method is certainly orders of magnitude less than competing neural syntax repair models.

- Line 999 “would significantly improve”: why?

Adding more contextual information such as edit history, compiler errors and other signals would surely allow the model to make better predictions. By equalizing all names and numbers, we are destroying a signal that is highly predictive of intent. Whether a simple Markov model could leverage this signal is unclear, but certainly a more sophisticated model could exploit this information to improve repair quality.